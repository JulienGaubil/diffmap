# Defines default config.
defaults:
  - data: llff
  - diffusion_model@model: ddpm
  - diffusion_model/model@model.params.unet_config: unet
  - diffusion_model/scheduler@model.params.scheduler_config: linear
  - diffusion_model/loss@model.params.flowmap_loss_config: flowmap # flowmap loss config

# Overrides image size in data and model children configs.
image_size: 128


model:
  params:
    log_every_t: 200 #when logging generated samples, frequency of logging for intermediate samples  
    timesteps: 1000 #sampling timesteps in forward/backward
    channels: 6 #number of noisy channels as input of U-Net
    conditioning_key: concat #defines the type of conditioning ie (crossattn, concat, hybrid)
    parameterization: eps
    unet_trainable: all  #defines trainable modules (all=True, attn, conv_in, conv_io, conv_out, conv_io_attn, false)
    compute_weights: True
    modalities_in: #diffusion modalities
    - trgt
    - optical_flow
    modalities_out: #modalities as output of the U-Net
    - trgt
    - optical_flow
    - depth_trgt
    - depth_ctxt
    ckpt_path: null

    unet_config:
      params:
        in_channels: 9 #number of input channels U-Net, should be multiple of 4 (original size)
        out_channels: 12 #number of output channels U-Net, should be multiple of 4 (original size)
        model_channels: 320 #number of intermediate layer channels across the U-Net
        attention_resolutions: [ 4, 2, 1]
        # use_spatial_transformer: True
        # transformer_depth: 1
        # context_dim: 768  #dim of crossattn conditioning

data:
  params:
    train:
      params:
        scenes: null
    validation:
      params:
        scenes: null


lightning:
  find_unused_parameters: true

  modelcheckpoint:
    params:
      every_n_train_steps: 1000

  callbacks: #configs for image logger
    image_logger:
      target: main.ImageLoggerDiffmap
      params:
        batch_frequency: 400 #frequency of logging images (counted in iterations) for train and val
        max_images: 4 #number of samples logged
        increase_log_steps: False
        log_all_val: True # permet de logger la validation, sinon ne log pas
        log_first_step: True
        log_images_kwargs:
          use_ema_scope: False
          inpaint: False
          plot_progressive_rows: False #defines whether logging intermediate steps when sampling images
          plot_diffusion_rows: False #defines whether logging diffusion forward process
          N: 4 #number of reconstructions/samples logged
          unconditional_guidance_scale: 3.0 #classifier-free guidance coeff
          unconditional_guidance_label: [""]

  trainer:
    gpus: [0]
    accelerator: ddp

    benchmark: True
    # check_val_every_n_epoch: null #frequency of validation in epochs
    # val_check_interval: null #frequency of validation in iterations
    # limit_val_batches: 0.0  # to disable validation
    num_sanity_val_steps: 0
    accumulate_grad_batches: 4
    max_epochs: 4000 #max number of epochs performed, 1000 is the default
    max_steps: 30000  #max number of iterations performed, 1000 is the default


experiment_cfg:
  name: overfit # name for log folder
  resume: null # path to checkpoint folder or .ckpt file

  postfix: ""
  logdir: logs/
  seed: 23
  finetune_from: ""  #SD checkpoint
  debug: False
  scale_lr: False
  train: True
  no_test: False

  trainer_config:
    # TODO print trainer_config
    gpus: [0]
    num_nodes: 1
    resume_from_checkpoint: ""