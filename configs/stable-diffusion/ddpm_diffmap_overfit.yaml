model:
  base_learning_rate: 1.0e-04
  target: ldm.models.diffusion.ddpm_diffmap.DDPMDiffmap
  params:
    linear_start: 0.00085
    linear_end: 0.0120
    num_timesteps_cond: 1
    log_every_t: 200 #when logging generated samples, frequency of logging for intermediate samples  
    timesteps: 1000 #sampling timesteps in forward/backward
    first_stage_key: "trgt" #key in batch for the noised signal
    cond_stage_key: "ctxt" #key in batch for conditioning signal
    image_size: 64 #2D resolution in the latent space, also controls sampling resolution (x8)
    channels: 0 #number of noisy image channels as input of U-Net
    cond_stage_trainable: false   # Note: different from the one we trained before
    conditioning_key: concat #defines the type of conditioning ie (crossattn, concat, hybrid)
    monitor: null
    parameterization: x0
    unet_trainable: all  #defines trainable modules (all=True, attn, conv_in, conv_io, conv_out, conv_io_attn)
    compute_weights: true
    modalities_in: []
    # - trgt
    # - optical_flow
    # - depth_trgt
    # - depth_ctxt
    modalities_out: 
    - depth_trgt
    - depth_ctxt
    ckpt_path: null

    scheduler_config: # 1000 warmup steps
      target: ldm.lr_scheduler.LambdaLinearScheduler
      params:
        warm_up_steps: [ 1000 ]
        cycle_lengths: [ 10000000000000 ] # incredibly large number to prevent corner cases
        f_start: [ 1.e-6 ]
        f_max: [ 1. ]
        f_min: [ 1. ]

    unet_config:
      target: ldm.modules.diffusionmodules.openaimodel.UNetModel
      params:
        image_size: 64 # unused
        in_channels: 3 #number of input channels U-Net, should be multiple of 4 (original size)
        out_channels: 6 #number of output channels U-Net, should be multiple of 4 (original size)
        model_channels: 320
        attention_resolutions: [ 4, 2, 1]
        num_res_blocks: 2
        channel_mult: [ 1, 2, 4, 4 ]
        num_heads: 8
        use_checkpoint: True
        legacy: False

    cond_stage_config:
      target: ldm.modules.encoders.modules.FrozenCLIPImageEmbedder
        
    flowmap_loss_config:
      # flow:
      #   name: raft
      #   num_flow_updates: 32
      #   max_batch_size: 8
      #   show_progress_bar: true

      model:
        use_correspondence_weights: true
        intrinsics:
          name: softmin
          num_procrustes_points: 8192
          min_focal_length: 0.5
          max_focal_length: 2.0
          num_candidates: 60
          regression: null
        extrinsics:
          name: procrustes
          num_points: 1000
          randomize_points: false

      loss:
        flow:
          weight: 10.0
          enable_after: 0
          mapping:
            name: huber
            delta: 0.01

      model_wrapper:
        patch_size: 32 #a modifier?

      cropping:
        image_shape: [64, 64] #a modifier?
        flow_scale_multiplier: 4
        patch_size: 32 #a modifier?


#params datamodules and datasets
data:
  target: main.DataModuleFromConfig
  params:
    batch_size: 4
    num_workers: 4
    num_val_workers: 0 # Avoid a weird val dataloader issue
    train: #params train dataset
      target: ldm.data.llff.LLFFDiffmapDataset
      params:
        root_dir: datasets/llff
        scenes:  #scenes used
        - horns
        n_val_samples_scene: 4  #number of val pairs per scene
        split: train
        image_transforms:
        - target: torchvision.transforms.Resize
          params:
            size: 64 #train img resolution
            interpolation: 3
        - target: torchvision.transforms.CenterCrop
          params:
            size: 64
    validation: #params val dataset
      target: ldm.data.llff.LLFFDiffmapDataset
      params:
        root_dir: datasets/llff
        split: val
        scenes:  #scenes used
        - horns
        n_val_samples_scene: 4  #number of val pairs per scene
        image_transforms:
        - target: torchvision.transforms.Resize
          params:
            size: 64 #val img resolution
            interpolation: 3
        - target: torchvision.transforms.CenterCrop
          params:
            size: 64

lightning:
  find_unused_parameters: false
  modelcheckpoint:
    params:
      every_n_train_steps: 1000
  callbacks: #configs for image logger
    image_logger:
      target: main.ImageLoggerDiffmap
      params:
        batch_frequency: 400 #frequency of logging images (counted in iterations) for train and val
        max_images: 4 #number of samples logged
        increase_log_steps: False
        log_all_val: True # permet de logger la validation, sinon ne log pas
        log_first_step: True
        log_images_kwargs:
          use_ema_scope: False
          inpaint: False
          plot_progressive_rows: False #defines whether logging intermediate steps when sampling images
          plot_diffusion_rows: False #defines whether logging diffusion forward process
          N: 4 #number of reconstructions/samples logged
          unconditional_guidance_scale: 0.0 #classifier-free guidance coeff
          unconditional_guidance_label: [""]

  trainer:
    benchmark: True
    # check_val_every_n_epoch: 30 #frequency of validation in epochs
    # val_check_interval: 5 #frequency of validation in iterations
    limit_val_batches: 0.0  # to disable validation
    num_sanity_val_steps: 0
    accumulate_grad_batches: 4
    max_epochs: 1000 #max number of epochs performed, 1000 is the default
    max_steps: 15000  #max number of iterations performed, 1000 is the default