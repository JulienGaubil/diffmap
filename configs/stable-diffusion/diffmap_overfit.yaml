model:
  base_learning_rate: 1.0e-04
  target: ldm.models.diffusion.ddpm.FlowMapDiffusion
  params:
    linear_start: 0.00085
    linear_end: 0.0120
    num_timesteps_cond: 1
    log_every_t: 200 #when logging generated samples, frequency of logging for intermediate samples  
    timesteps: 1000 #sampling timesteps in forward/backward
    first_stage_key: "trgt" #key in batch for the noised signal
    cond_stage_key: "ctxt" #key in batch for the clean conditioning signal
    image_size: 64 #2D resolution in the latent space, also controls sampling resolution (x8)
    channels: 16 #number of channels in latent space
    cond_stage_trainable: false   # Note: different from the one we trained before
    conditioning_key: crossattn
    monitor: val/loss_simple_ema
    scale_factor: 0.18215
    unet_trainable: conv_io_attn  #defines trainable modules (all=True, attn, conv_in, conv_io, conv_out, conv_io_attn)
    modalities:
    - trgt
    - optical_flow
    - depth_trgt
    - depth_ctxt
    ckpt_path: null

    scheduler_config: # 1000 warmup steps
      target: ldm.lr_scheduler.LambdaLinearScheduler
      params:
        warm_up_steps: [ 1000 ]
        cycle_lengths: [ 10000000000000 ] # incredibly large number to prevent corner cases
        f_start: [ 1.e-6 ]
        f_max: [ 1. ]
        f_min: [ 1. ]

    unet_config:
      target: ldm.modules.diffusionmodules.openaimodel.UNetModel
      params:
        image_size: 32 # unused
        in_channels: 16 #number of input channels U-Net, should be multiple of 4 (original size)
        out_channels: 16 #number of output channels U-Net, should be multiple of 4 (original size)
        model_channels: 320
        attention_resolutions: [ 4, 2, 1 ]
        num_res_blocks: 2
        channel_mult: [ 1, 2, 4, 4 ]
        num_heads: 8
        use_spatial_transformer: True
        transformer_depth: 1
        context_dim: 768  #dim of conditioning?
        use_checkpoint: True
        legacy: False

    first_stage_config:
      target: ldm.models.autoencoder.AutoencoderKL
      params:
        embed_dim: 4
        monitor: val/rec_loss
        ddconfig:
          double_z: true
          z_channels: 4
          resolution: 256
          in_channels: 3
          out_ch: 3
          ch: 128
          ch_mult:
          - 1
          - 2
          - 4
          - 4
          num_res_blocks: 2
          attn_resolutions: []
          dropout: 0.0
        lossconfig:
          target: torch.nn.Identity

    cond_stage_config:
      target: ldm.modules.encoders.modules.FrozenCLIPImageEmbedder
    
    first_stage_flow_config:
      base_learning_rate: 4.5e-06
      target: ldm.models.autoencoder.AutoencoderKL
      params:
        image_key: optical_flow #key of the target signal in the dataset samples dict
        ckpt_path: "/home/jgaubil/projects/diffmap/codes/stable-diffmap/logs/auto_encoder/auto_encoder_kl_f8_flow_lowres/checkpoints/last.ckpt"
        monitor: val/rec_loss
        embed_dim: 4
        lossconfig:
          target: ldm.modules.losses.LPIPSWithDiscriminator
          params:
            disc_start: 50001
            kl_weight: 1.0e-06
            disc_weight: 0.5
        ddconfig:
          double_z: true
          z_channels: 4
          resolution: 256
          in_channels: 3
          out_ch: 3
          ch: 128
          ch_mult:
          - 1
          - 2
          - 4
          - 4
          num_res_blocks: 2
          attn_resolutions: []
          dropout: 0.0

#params datamodules and datasets
data:
  target: main.DataModuleFromConfig
  params:
    batch_size: 4
    num_workers: 4
    num_val_workers: 0 # Avoid a weird val dataloader issue
    train: #params train dataset
      target: ldm.data.llff.LLFFDiffmapDataset
      params:
        root_dir: datasets/llff
        scenes:  #scenes used
        - horns
        n_val_samples_scene: 4  #number of val pairs per scene
        split: train
        image_transforms:
        - target: torchvision.transforms.Resize
          params:
            size: 512 #train img resolution
            interpolation: 3
        - target: torchvision.transforms.CenterCrop
          params:
            size: 512
    validation: #params val dataset
      target: ldm.data.llff.LLFFDiffmapDataset
      params:
        root_dir: datasets/llff
        split: val
        scenes:  #scenes used
        - horns
        n_val_samples_scene: 4  #number of val pairs per scene
        image_transforms:
        - target: torchvision.transforms.Resize
          params:
            size: 512 #val img resolution
            interpolation: 3
        - target: torchvision.transforms.CenterCrop
          params:
            size: 512

lightning:
  find_unused_parameters: false
  modelcheckpoint:
    params:
      every_n_train_steps: 1000
  callbacks: #configs for image logger
    image_logger:
      target: main.ImageLoggerDiffmap
      params:
        batch_frequency: 400 #frequency of logging images (counted in iterations) for train and val
        max_images: 4 #number of samples logged
        increase_log_steps: False
        log_all_val: True # permet de logger la validation, sinon ne log pas
        log_first_step: True
        log_images_kwargs:
          use_ema_scope: False
          inpaint: False
          plot_progressive_rows: False #defines whether logging intermediate steps when sampling images
          plot_diffusion_rows: False #defines whether logging diffusion forward process
          N: 4 #number of reconstructions/samples logged
          unconditional_guidance_scale: 3.0 #classifier-free guidance coeff
          unconditional_guidance_label: [""]

  trainer:
    benchmark: True
    check_val_every_n_epoch: 30 #frequency of validation in epochs
    # val_check_interval: 5 #frequency of validation in iterations
    # limit_val_batches: 0.0  # to disable validation
    num_sanity_val_steps: 0
    accumulate_grad_batches: 1
    max_epochs: 1000 #max number of epochs performed, 1000 is the default
    max_steps: 15000  #max number of iterations performed, 1000 is the default